\documentclass[../main.tex]{subfiles}
\begin{document}


\section{Neuronale Netze}

Neuronale Netzwerke (NNs) bilden die Basis für künstliche Intelligenz. Sie sind in ihrer Funktionsweise dem menschlichen Gehirn nachempfunden. Dementsprechend bestehen sie aus einer 
bestimmten Anzahl von Neuronen. Diese sind in verschiedenen Schichten angeordnet. Die erste dieser Schichten wird als Eingabeschicht bezeichnet, die letzte als Ausgabeschicht. Dazwischen 
befinden sich mehrere sogenannte versteckte Schichten. Jedes Neuron ist über gewichtete Verbindungen mit allen Neuronen aus vorherigen und der nachfolgenden Schicht verbunden. 
An jedem gegebenen Zeitpunkt hat ein Neuron einen bestimmten Wert zwischen 0 und 1, welcher dem Output des Neurons entspricht. Dieser Wert ergibt sich aus der Verarbeitung der Inputs welche 
aus den Outputs der vorherigen Schicht berechnet werden, das heißt es wird eine Funktion auf die Summe aller Inputs ausgeführt. \\
Jede Verbindung zwischen Neuronen verfügt über eine Gewichtung zwischen -1 und 1. Diese bestimmt, mit welcher Zahl der Wert eines Neurons multipliziert wird, bevor dieser an die Neuronen in 
der folgenden Schicht weitergegeben wird. Die Gewichtungen der Verbindungen zwischen zwei Schichten können als Matrix dargestellt werden. Der Übergang von einer 
Schicht zu der darauf folgenden entspricht einer Matrixmultiplikation. \\
Die Werte der Gewichtungen stehen zu Beginn noch nicht fest. Sie werden erst durch das sogenannte Training festgelegt. Das Training eines NNs beschreibt also die konkrete Anpassung der 
Gewichtungen. Dies wird durch Backpropagation erreicht. Dafür werden die Gewichtungen zunächst mit zufälligen Werten initialisiert. Dann wird der NN-Algorithmus mit einer Vielzahl an 
Trainingsdaten durchlaufen. Die erhaltenen werden mit den gewünschten Ergebnissen verglichen. Treten dabei zu starke Abweichungen auf, werden alle zu diesem Ergebnis beitragenden 
Gewichtungen reduziert, sodass sie weniger Einfluss auf die entstehenden Ergebnisse haben. Stimmen gewünschtes und erhaltenes Ergebnis annähernd überein, werden die dazu beitragenden 
Gewichtungen erhöht. NN bilden die Grundlage für weitere Technologien wie beispielsweise Large Language Models, welche auch für KI-Schreibwerkzeuge verwendet werden. 

\section{Large Language Models}

Large Language Models (LLMs) sind eine Form der KI, welche auf natürlicher Sprache basiert. Da sie entwickelt wurden, um Texte zu generieren, werden LLMs auch als generative KI bezeichnet.
Moderne LLMs wie ChatGPT basieren auf einer Transformer-Architektur. Diese berechnet aus einem Eingabetext eine Wahrscheinlichkeitsverteilung 
über alle möglichen nächsten Tokens. Die Transformerarchitektur setzt sich aus folgenden Aspekten zusammen:\\

\begin{itemize}

\item Tokenisierung: Als erster Schritt wird der Eingabetext in einzelne Tokens unterteilt. Tokens können Wörter oder Wortteile, aber auch Satzzeichen oder einzelne Buchstaben sein. Sie sind die kleinstmögliche Einheit, welche das LLM verarbeiten kann. Jedes Token hat eine Token-ID. 

\item Embedding: Den Tokens werden vieldimensionale Vektoren zugeordnet. So werden sie anhand ihrer Bedeutung codiert. Die Richtungen der Vektoren im Vektorraum beinhalten semantische Bedeutungen. So liegen Wörter mit ähnlicher Bedeutung, beispielsweise Synonyme, nah beieinander.

\item Attention: Die einzelnen als Vektoren codierten Tokens werden durch den umliegenden Text verändert, um den Kontext des Wortes und dessen semantische Bedeutung widerzuspiegeln. Dabei werden nacheinander die einzelnen Vektoren betrachtet und ein Skalarprodukt aus dem aktuellen und jeweils den anderen Vektoren des Textes gebildet. Das Skalarprodukt beschreibt die Relevanz des anderen Tokens für die Bedeutung des aktuellen Tokens. Je größer dieser Bedeutungwert ist, desto mehr wird der aktuelle Vektor auf Grundlage des anderen Vektors verändert.\\
Die Attention-Funktion ist vollständig linear. Dieser Mechanismus lässt sich an folgendem Beispiel verdeutlichen: In den beiden Sätzen “Ich sitze auf der Bank” und “Ich habe in der Bank Geld abgehoben” hat das Wort “Bank” eine andere Bedeutung. Der Attention-Mechanismus passt die Vektoren, welche dieses Token codieren, so an, dass sie jeweils näher an den entsprechenden Synonymen “Parkbank” und “Kreditinstitut” liegen. Durch diese Codierung bleibt der Kontext, in welchem ein Wort verwendet wird, erhalten. 

\item Multi-Layer-Perceptron: Im Gegensatz zur ausschließlich linearen Attention-Funktion, kann das LLM mithilfe des Multi-Layer-Perceptrons (MLP) auch nicht-lineare Zusammenhänge darstellen. Das MLP ist ein kleines neuronales Netzwerk, meistens mit nur einer inneren Schicht sowie der Eingabe-und Ausgabeschicht. Jeder Vektor wird einmal als Input in dieses neuronale Netzwerk eingegeben und der Output zu dem ursprünglichen Vektor addiert. Mithilfe des MLPs kann die Komplexität der Sprache erfasst werden.

\item Decoder: Die Architektur des LLMs besteht aus mehreren Attention- und MLP-Schichten, welche abwechselnd hintereinander angeordnet sind. Abschließend verfügt es über einen Decoder, welcher den Output aus der letzten MLP-Schicht als Input bekommt. Darauf wird eine Softmax-Funktion angewendet, welche eine Wahrscheinlichkeitsverteilung über alle Tokens ausgibt. Die Softmax-Funktion sorgt dafür, dass die Summe der Wahrscheinlichkeiten 1 beträgt, und jede einzelne Wahrscheinlichkeit im Intervall von 0 bis 1 liegt. 
\end{itemize}


Entsprechend der sich aus diesem Prozess ergebenden Wahrscheinlichkeitsverteilung wird in einem LLM ein zufälliges nächstes Wort ausgewählt und an den generierten Text angefügt. 
Der Prozess wird anschließend mit dem aktualisierten Text wiederholt, sodass immer neue Tokens angefügt werden, bis eine Abbruchbedingung erreicht ist. Diese kann unter anderem das 
Erreichen der vom Nutzer festgelegten maximalen Anzahl generierter Tokens oder die Generierung eines Abbruchtokens vom LLM selbst sein.\\

Mit dieser Architektur können erstaunliche Ergebnisse erzielt werden. Die generierten Texte ähneln menschengeschriebenen Texten sehr, wodurch sich viele Anwendungsgebiete für diese Technologie ergeben. Jedoch ergeben sich daraus auch einige Probleme und Risiken, welche im Folgenden erläutert werden sollen. 

\section{Probleme und Risiken der Nutzung von KI}

\subsection{Abhängigkeit von den Trainingsdaten}

Da KI Zusammenhänge lediglich auf Grundlage der verwendeten Trainingsdaten erlernt, sind diese häufig die Ursache für Probleme. Werden beispielsweise Trainingsdaten verwendet, 
bei denen bestimmte Personengruppen benachteiligt werden oder seltener Vorkommen, kann das die generierten Inhalte der KI beeinflussen. Diskriminierende Inhalte können aus den Trainingsdaten 
übernommen werden. Zudem können sich durch fehlende Informationen über bestimmte Personengruppen, Sprachen, Dialekte etc. auch Nachteile in der Nutzung von KI für betroffene 
Personengruppen ergeben.\\ Dieses Verhalten generativer KI kann sowohl soziale als auch wirtschaftliche Folgen haben. KI Anbieter versuchen zwar, diesem Problem durch eine möglichst 
facettenreiche Auswahl von Trainingsdaten und dem Einbau von Filtern, welche beispielsweise das Auftreten bestimmter Wörter wie Beleidigungen verhindern, diesem Problem entgegenzuwirken, 
dennoch sollten sich Nutzer diesem Risiko bewusst sein.\\

\subsection{Halluzinationen}

Die Texte, welche eine KI ausgibt, können falsche Informationen beinhalten. Diese werden häufig als "`Halluzinationen"' bezeichnet. KI-Halluzination beschreibt 
das Phänomen, dass generative KI-Anwendungen Antworten erzeugen, welche zwar plausibel erscheinen, jedoch in Wirklichkeit unlogisch oder unzutreffend sind\cite{hallucinationForewarning}.\\
Halluzinationen können verschiedene Ursachen haben. Auch dafür bilden Probleme mit den Trainingsdateninhalten einen potentiellen Grund. Sind die Trainingsdaten schon älter,
können sie veraltete Informationen enthalten. Durch das Fehlen aktueller Daten kann das KI-Modell keine korrekten Aussagen zu aktuellen Ereignissen tätigen.\\ Des 
Weiteren können die Daten ungenau, nicht fallspezifisch oder inkorrekt sein. Besonders bei einem Modelltraining mit Daten aus dem Internet besteht die Gefahr, dass 
Trainingsdaten Fehler enthalten.\\ Als weitere potenzielle Ursache für Halluzinationen kommen Schwachstellen in der Architektur in Betracht. Als "`Sycophancy"' wird das 
Phänomen beschrieben, dass das KI-Modell Texte generiert, welche den Erwartungen des Nutzers entsprechen, ohne dabei auf fachliche Korrektheit zu achten. Dies kann ebenfalls 
durch die Formulierung des Prompts beeinflusst werden.\cite{allgemHalluzinationen} \\
Ferner birgt die Methode, mit welcher KI Texte generiert, ein Risiko für Falschinformationen. Beispielsweise wird durch eine hohe Temperatureinstellung die 
Wahrscheinlichkeit für unplausible oder falsche Inhalte größer. Ein Artikel der Frauenhofer-Institutes beschreibt, dass gewisse Token-Typen sehr nah beieinander liegen 
und somit auch ähnliche Wahrscheinlichkeiten haben. Dazu zählten zum Beispiel "`ähnliche numerische Werte wie Preise (9,99 EUR; 10,00 EUR), nahe beieinander liegende 
Daten (2020, 2021), ähnlich klingende Namen, oder technische Begriffe und Abkürzungen (KI, ML)"'\cite{halluzinationenFraunhofer}. Eine Verwechslung dieser Daten kann ebenfalls zu inkorrekten Aussagen 
führen.\\
Darüber hinaus können bei der Auswahl des nächsten Tokens aufgrund der Wahrscheinlichkeit, wie in Abschnitt xxx beschrieben, unpassende Wörter bevorzugt werden, welche 
daraufhin die Grundlage für Halluzinationen bilden. Technisch begründet ist dieses Phänomen mit dem sogenannten Softmax-Bottleneck. Bei der Generierung der Liste von 
Wahrscheinlichkeiten für das nächste Wort wird der Softmax-Algorithmus auf einen mehrdimensionalen Vektor angewendet. Aufgrund der Natur dieses Algorithmus kann nur 
ein Ausschnitt aller möglichen Wahrscheinlichkeitsverteilungen dargestellt werden. Dadurch kann es passieren, dass die Verteilung nicht korrekt abgebildet und 
unpassenden Wörtern eine höhere Wahrscheinlichkeit zugeschrieben wird.\cite{softmax} \\
Ein so gewähltes unpassendes Wort, welches dem Text angefügt wird, bildet wiederum die Grundlage für das Generieren der darauffolgenden Wörter. Sobald in einem KI-Chat eine 
bestimmte Falschinformation auftritt, kann die sogenannte "`Over-Confidence"' dazu führen, dass auch bei wiederholtem Nachfragen oder versuchtem Korrigieren der 
Aussage, das KI-Modell weiterhin auf die Korrektheit der Behauptungen besteht. Dies erschwert das Überprüfen der Fakten für den Anwender.\cite{allgemHalluzinationen,softmax} \\
Selbst wenn ein KI-generierter Text keine Falschinformationen enthält, können die beschriebenen Phänomene eine Unvollständigkeit der generierten Texte hervorrufen 
oder zu einem Abweichen von der eigentlichen Fragestellung führen.  Das als "`Instructions-Forgetting"' bekannte Phänomen beschreibt, dass eine KI den Kontext der 
ursprünglichen Anfrage vergisst und einen Text generiert, der inhaltlich nicht der Fragestellung entspricht. Deswegen sollten KI-generierte Texte, besonders im Kontext 
des wissenschaftlichen Schreibens, immer überprüft werden.\cite{allgemHalluzinationen}


\subsection{Erklärbarkeitsproblem}

blaBlalalablablblbabla lba bla bla blaBlalalablablblbabla

\subsection{Ressourcenverbrauch}

KI-Modelle verbrauchen sowohl während des Trainings als auch beim Bearbeiten der Anfragen viele Ressourcen. Desto größer das verwendete Modell, desto höher ist auch der Ressourcenverbrauch. 
Der Stromverbrauch von GPT4 0.1 liegt laut eines im Jahr 2025 veröffentlichten Artikels bei bis zu 1 KWH pro Anfrage\cite{Energieverbrauch}. Für die oben beschriebene Anpassung der Parameter sind während des Bearbeitens einer Anfrage viele 
komplexe Berechnungen in möglichst kurzer Zeit notwendig. Auch das Training ist durch die große Menge benötigter Daten und die häufig wochen- bis monatelangen Trainingszeiten 
energieintensiv. In den Rechenzentren wird zudem Wasser zur Kühlung verwendet. Der Wasser-und Ressourcenverbrauch der Nutzung eines KI-Modells ist im Vergleich zu dem der Nutzung einer 
Suchmaschine wie Google sehr hoch\cite{KINachhaltigkeit}. 

\subsection{Datenschutz}

Zudem besteht beim Training sowie bei der Nutzung von KI das Problem des Datenschutzes. Die Trainingsdaten können personenbezogene Daten beinhalten. Das KI-Modell kann bestimmte sensible 
Informationen aus den Trainingsinhalten wiedergeben, auch wenn diese nicht absichtlich gespeichert werden sollen. Einige LLM-Anbieter behalten sich das Recht vor, das Modell anhand der 
Eingabedaten weiter zu trainieren und die Daten anderweitig zu nutzen. Auch so können personenbezogenen oder unternehmensspezifischen Informationen in die Trainings- oder Ausgabedaten 
erscheinen und so zu Datenschutzverletzungen führen. Im Falle einer missbräuchlichen Verwendung solcher Daten ist oft unklar, wer dafür die Verantwortung trägt. Vor allem bei der KI-Nutzung 
über Online-Schnittstellen sollte daher sorgfältig mit wichtigen Informationen umgegangen werden. Sie sollten beispielsweise nicht in Form des Promptes an das Modell gegeben werden.

\end{document}